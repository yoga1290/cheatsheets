{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![install](\n",
    "https://anaconda.org/yoga1290/pytorch/badges/installer/ipynb.svg)](https://anaconda.org/yoga1290/pytorch/notebook) [![downloads](https://anaconda.org/yoga1290/pytorch/badges/downloads.svg)](https://anaconda.org/yoga1290/pytorch/notebook) [![Anaconda Cloud](\n",
    "https://anaconda.org/yoga1290/pytorch/badges/version.svg)](https://anaconda.org/yoga1290/pytorch/notebook)\n",
    "# NOTE: WIP, visit me later!\n",
    "[![Open in CoLab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yoga1290/cheatsheets/blob/master/PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "+ [Setup](#Environment-setup)\n",
    "+ **Training**: adjust W & b\n",
    "    + Initialization\n",
    "        + [Dataset & DataLoader](#Dataset)\n",
    "        + **Ecoph**: iteration in the training phase\n",
    "    + Forward Propagation\n",
    "        + [Model](#Models)\n",
    "            + **in_features**: size of W\n",
    "            + **out_features**: classes\n",
    "            + Linear Classifiers: returns positive & negative values\n",
    "            + Logistic Regression: returns [0 - 1] values\n",
    "            + Threshold function: returns either 0 or 1 \n",
    "            + [Linear Regression](#Linear-Regression)\n",
    "            + Autoencoders\n",
    "                + Shallow Neural Networks\n",
    "                + Uses deterministic approach\n",
    "                + Dimensionality Reduction\n",
    "                + **RBM: Restricted Boltzman Machine**\n",
    "                    + Shallow Neural Networks (2 layers)\n",
    "                    + feature extraction\n",
    "                    + Uses a stochastic approach\n",
    "                    + Generative model\n",
    "                        + \"We can do both supervise and unsupervised tasks with generative models\" [[ref](https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/ML0120EN/ML0120EN-4.1-Review-RBMMNIST.ipynb)]\n",
    "                    + 2 Phase model [[ref](https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/ML0120EN/ML0120EN-4.1-Review-RBMMNIST.ipynb)]:\n",
    "                        + Forward pass: $p(h|v) = sigmoid(X \\otimes W + hb)$\n",
    "                        + Backward Pass/Reconstruction: $p(v|h) = sigmoid(h0 \\otimes transpose(W) + vb)$\n",
    "                    + **DBN: Deep Believe Network**\n",
    "                        + feature extraction is unsupervised by a stack of RBMs.\n",
    "                        + Solves the backward propagation, local minima & vanishing gradients\n",
    "            + **Recurrent Model/RNN**\n",
    "                + May use time-window to store state\n",
    "                + Vanishing/Exploding gradient may occurs\n",
    "                + **LSTM: Long Short Term Memory networks**\n",
    "                    + Keep/Read/Write Gates\n",
    "                    + solves the vanishing/exploding gradient\n",
    "                    + Unfloaded LSTM\n",
    "                    + Stacked LSTM\n",
    "                    \n",
    "            + **Weight Initialization**\n",
    "                + Zeros\n",
    "                    + the derivative with respect to loss function is the same for every w; similar to linear model [read me](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "                + **Xavier initialization** [see](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "                    + Tanh\n",
    "                + He/Kaiming uniform initialization\n",
    "                    + ReLU\n",
    "                + Uniform distribution\n",
    "                + Normal distribution\n",
    "                    + Vanishing gradients\n",
    "                    + Exploding gradients\n",
    "                        + \"This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn\" [see](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "        + **Normalization**:\n",
    "            + \"Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information\" [see](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/normalize-data#module-overview)\n",
    "            + Batch Normalization\n",
    "                + \"batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\" [see](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c#b87b)\n",
    "                + Remove Dropout\n",
    "                + Reduce Internal Covariate Shift\n",
    "                + Increase learning rate\n",
    "                + Bias is not necessary\n",
    "                + USE `model.eval()` before perdication ($\\hat{y}$)\n",
    "        + **Regularization** [[ref](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)] [[gDev L1](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization)] [[gDev L2](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization)]\n",
    "            + prevent overfitting by penalizing complex models\n",
    "            + \"Zeroing out features will save RAM and may reduce noise in the model\" [[gDev L1](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization)]\n",
    "            + [Structural risk minimization](https://developers.google.com/machine-learning/glossary/#SRM)\n",
    "            + [Google Playground](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/playground-exercise)\n",
    "            + L0\n",
    "                + \"it would turn our convex optimization problem into a non-convex optimization problem that's NP-hard\"\n",
    "            + L1 Regularization\n",
    "                + Loss += $\\lambda \\sum | \\beta | $\n",
    "                + \"shrinks the less important featureâ€™s coefficient to zero\"\n",
    "                + \"works well for feature selection in case we have a huge number of features\"\n",
    "                + \"to exactly 0 where possible\"\n",
    "                + \"L2 regularization encourages weights to be small, but doesn't force them to exactly 0.0\"\n",
    "                + constant \n",
    "                + Lasso Regression\n",
    "                + LASSO: Least Absolute Shrinkage and Selection Operator\n",
    "            + L2 Regularization\n",
    "                + Ridge Regression\n",
    "                + Loss += $ \\lambda \\sum \\beta ^2 $\n",
    "                + [Interactive example](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization) [Example](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization)\n",
    "            + Frobenius norm\n",
    "            + Dropout\n",
    "                + USE `model.train()` before training\n",
    "                + USE `model.eval()` before perdication ($\\hat{y}$)\n",
    "        + **Activation functions**:\n",
    "            + Tanh\n",
    "                + zero centered [-1, 1]\n",
    "                + **Xavier initialization** [see](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "                + Vanishing gradient\n",
    "            + Sigmoid\n",
    "                + [0, 1]\n",
    "                + Initialization\n",
    "                + Vanishing gradient\n",
    "                + **Binary** classification\n",
    "            + ReLU\n",
    "                + [0, 1]\n",
    "                + \"With RELU(z) vanishing gradients are generally not a problem as the gradient is 0 for negative (and zero) inputs and 1 for positive inputs.\" [read me](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "            + Softmax\n",
    "                + **Multi-class** classification\n",
    "                \n",
    "    + **Loss/Cost**: the difference between the prediected values,\n",
    "        $\\hat{y}$ and true labels, $y$\n",
    "        + [Derivative](#Derivative)\n",
    "        + [Mean Square Error](#Mean-Square-Error)\n",
    "        + [Binary Cross Entropy](#Binary-Cross-Entropy)\n",
    "        + [Cross Entropy](#Cross-Entropy)\n",
    "            + **Multi-class** \"This criterion expects a class index (0 to C-1) as the target for each value\" [PyTorch](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "        + \n",
    "    + **Backward propagation**:    \n",
    "        + **Optimization**: updates W & b in the Backward propagation\n",
    "            + [Adam optimizer](#Adam)\n",
    "            + [Gradient Descent Optimization](#Gradient-Descent)\n",
    "                + Batch Gradient Descent [[Ruder.io](http://ruder.io/optimizing-gradient-descent/index.html#batchgradientdescent)]\n",
    "                + Mini-Batch Gradient Descent (PyTorch's **default**)\n",
    "                + Stochastic Gradient Descent\n",
    "                    + Update loss by one sample at a time\n",
    "                    + Sudden increases may occur\n",
    "                    + May not be accurate\n",
    "                    + Good for big data\n",
    "+ **Validation**: adjust the hyper-parameters; learning rate & batch size\n",
    "    + **Early Stopping**:\n",
    "        + \"Stop training when a monitored quantity has stopped improving\" [[tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)]\n",
    "        + or validation error just got worse\n",
    "\n",
    "+ Tools & Libraries\n",
    "    + **Visualization**\n",
    "        + Pandas\n",
    "        + Matplotlib\n",
    "    + [Numpy](#NumPy)\n",
    "+ Cheatsheets\n",
    "    + [ml-cheatsheet.readthedocs.io](https://ml-cheatsheet.readthedocs.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "\n",
    "## Install\n",
    "+ Install [Anaconda](https://www.anaconda.com/download/#linux)\n",
    "+ Expose `~/anaconda3/bin` (where `conda` executable biniary)\n",
    "+ Install [PyTorch](https://pytorch.org/): `conda install pytorch torchvision -c pytorch`\n",
    "\n",
    "## Online tools\n",
    "\n",
    "### [Google CoLaboratory](https://colab.research.google.com) [[open me!](https://colab.research.google.com/github/yoga1290/cheatsheets/blob/master/PyTorch.ipynb)]\n",
    "\n",
    "Install PyTorch on [Google CoLaboratory](https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb#scrollTo=RHXKNvj8ROgq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb#scrollTo=RHXKNvj8ROgq\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import arange, randn\n",
    "\n",
    "# https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel#dataset\n",
    "class MyDataset(Dataset):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.x = arange(-3, 3, 0.1).view(-1, 1)\n",
    "        self.f = 1 * self.x - 1\n",
    "        self.y = self.f + 0.1 * randn(self.x.size())\n",
    "        self.len = self.x.shape[0]\n",
    "        \n",
    "    # Getter\n",
    "    def __getitem__(self,index):    \n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "#ds = MyDataset()\n",
    "# ds[index] to __getitem__(index)\n",
    "\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "dataLoader = DataLoader(MyDataset(), **params)\n",
    "\n",
    "# dataLoader = DataLoader(dataset=MyDataset(), batch_size=64, shuffle=True, num_workers=6)\n",
    "\n",
    "# for X, y in dataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prebuilt dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "dataset = dsets.MNIST(\n",
    "    root = './data2', \n",
    "    train = False, \n",
    "    download = True, \n",
    "    transform = transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "def show_data(data_sample, shape = (28, 28)):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1].item()))\n",
    "\n",
    "show_data(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision Transforms\n",
    "\n",
    "+ Compose\n",
    "+ CenterCrop\n",
    "+ ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "croptensor_data_transform = Compose([\n",
    "    CenterCrop(20),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# set train = false for validation\n",
    "dataset = dsets.MNIST(root = './data', train = False, download = True, transform = croptensor_data_transform)\n",
    "print(\"The shape of the first element in the first tuple: \", dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "![source: https://youtu.be/zJSY2C9xzoU](https://github.com/yoga1290/cheatsheets/raw/4346bd63cf490d978cee156497db3b64f87fc37e/resources/nn-3.3.2-zJSY2C9xzoU.png)\n",
    "![source: https://youtu.be/zJSY2C9xzoU](https://github.com/yoga1290/cheatsheets/raw/4346bd63cf490d978cee156497db3b64f87fc37e/resources/nn-3.3.2-2-zJSY2C9xzoU.png)\n",
    "![source: https://youtu.be/xR4Ian1UIGM](https://github.com/yoga1290/cheatsheets/raw/4346bd63cf490d978cee156497db3b64f87fc37e/resources/nn2-xR4Ian1UIGM.png)\n",
    "> sources [[1](https://youtu.be/xR4Ian1UIGM)] [[2](https://youtu.be/xR4Ian1UIGM)] [[3](https://youtu.be/xR4Ian1UIGM)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, Sigmoid\n",
    "\n",
    "model = Sequential( Linear(in_features, out_features, bias=True), Sigmoid(), ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.init.kaiming_uniform_(linear.weight,nonlinearity='relu')\n",
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/DL0110EN/5.1.2.He_Initialization.ipynb\n",
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/DL0110EN/5.3.1BachNorm.ipynb\n",
    "\n",
    "from torch.nn import Module, ModuleList, Linear, Dropout, BatchNorm1d\n",
    "from torch.nn.init import kaiming_uniform_, xavier_uniform_\n",
    "from torch.nn.functional import relu, tanh\n",
    "\n",
    "class Net(Module):\n",
    "    # Constructor\n",
    "    # in_features = len(W)\n",
    "    def __init__(self,Layers, p=0):\n",
    "        # Inherit from parent\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden = ModuleList()\n",
    "        self.drop = Dropout(p=p)\n",
    "\n",
    "        for input_size,output_size in zip(Layers,Layers[1:]):\n",
    "            linear = Linear(input_size,output_size)\n",
    "            \n",
    "            # Uniform initialization\n",
    "            #linear.weight.data.uniform_(0, 1)\n",
    "            \n",
    "            # He/Kaiming uniform initialization\n",
    "            #kaiming_uniform_(linear.weight, nonlinearity='relu')\n",
    "            \n",
    "            # Xavier initialization\n",
    "            #xavier_uniform_(linear.weight)\n",
    "            \n",
    "            self.hidden.append( linear )\n",
    "            \n",
    "            # Batch Normalization\n",
    "            # self.hidden.append( BatchNorm1d(output_size) )\n",
    "            \n",
    "            # Dropout\n",
    "            # self.hidden.append( Dropout(p=p) )\n",
    "\n",
    "    # Prediction function\n",
    "    # https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "    def forward(self,x):\n",
    "        L=len(self.hidden)\n",
    "        for (l, linear_transform)  in zip(range(L),self.hidden):\n",
    "            if l<L-1:\n",
    "                x = relu(linear_transform (x))\n",
    "                #x = tanh(linear_transform (x))\n",
    "                #x = self.drop(x)\n",
    "            # last layer\n",
    "            else:\n",
    "                x =linear_transform (x)\n",
    "        \n",
    "        return x #yhat\n",
    "    \n",
    "    #def activation(self,x):\n",
    "    #    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### state_dict(), load_state_dict(dict), save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from torch import load\n",
    "from torch.nn import Module, Linear\n",
    "\n",
    "save({\"a\": 123}, 'tmp.pt')\n",
    "tdict = load('tmp.pt')\n",
    "print(tdict)\n",
    "\n",
    "model = Linear(5, 1)\n",
    "save(model.state_dict(), 'model.pt')\n",
    "model.load_state_dict( load('model.pt') )\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linspace, tensor\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "x = linspace(-3, 3, 100, requires_grad = True)\n",
    "Y = relu(x)\n",
    "\n",
    "z = tensor([[1,0,-1],[2,0,-2],[1,0,-1]])\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterion/Cost/Loss\n",
    "\n",
    "Comparing/differentiating the prediected values (**Y^**) and the actual labels (**Y**)\n",
    "\n",
    "### Mean Square Error\n",
    "\n",
    "+ [torch.nn.MSELoss(size_average=None, reduce=None, reduction='elementwise_mean')](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "criterion = MSELoss()\n",
    "\n",
    "# equivalent to:\n",
    "from torch import mean\n",
    "\n",
    "def criterion(yhat, y):\n",
    "    return mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "\n",
    "criterion = BCELoss()\n",
    "\n",
    "# equivalent to:\n",
    "from torch import mean\n",
    "from torch import log\n",
    "\n",
    "def criterion(yhat, y):\n",
    "    return -1 * mean(y * log(yhat) + (1-y) * log(1 - yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "# https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "+ [ruder.io](http://ruder.io/optimizing-gradient-descent/index.html)\n",
    "\n",
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "+ [Overview of mini-batch gradient descent](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n",
    "\n",
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "optimizer = SGD(model.parameters(),lr=learning_rate) # momentum=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "+ [PUML](https://raw.githubusercontent.com/yoga1290/cheatsheets/master/gradient-descent.puml)\n",
    "![Gradient Descent](https://github.com/yoga1290/cheatsheets/raw/master/gradient-descent.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train([true]) # sets model.training = true\n",
    "\n",
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/DL0110EN/5.1.1Xaviermist1layer.ipynb\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch import mean # works for Tensors\n",
    "from numpy import mean\n",
    "# from tensorflow import reduce_mean\n",
    "\n",
    "def train(model,\n",
    "          criterion,\n",
    "          train_loader,\n",
    "          validation_loader,\n",
    "          optimizer,\n",
    "          epochs=100):\n",
    "    \n",
    "    result = {'training_loss':[],'validation_accuracy':[]}\n",
    "    \n",
    "    #n_epochs\n",
    "    for epoch in range(epochs):\n",
    "        loss_sublist = []\n",
    "        #for i,(x, y) in enumerate(train_loader): # to print(i)\n",
    "        for x, y in train_loader:\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "            #clear gradient \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #make a prediction logits \n",
    "            z = model(x.view(-1, 28*28)) # TODO\n",
    "            \n",
    "            # calculate loss \n",
    "            loss = criterion(z, y)\n",
    "    \n",
    "            # calculate gradients of parameters \n",
    "            loss.backward()\n",
    "            \n",
    "            # update parameters \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sublist.append(loss.data.item())\n",
    "            \n",
    "        result['training_loss'].append(mean(loss_sublist))\n",
    "        \n",
    "        correct = 0\n",
    "        for x, y in validation_loader:\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            #perform a prediction on the validation  data  \n",
    "            z = model(x.view(-1, 28*28)) #TODO\n",
    "            \n",
    "            _, yhat = torch.max(z, 1)\n",
    "            correct += (yhat == y).sum().item()\n",
    "\n",
    "            \n",
    "        accuracy = 100*(correct/len(validation_dataset))\n",
    "        result['validation_accuracy'].append(accuracy)\n",
    "    \n",
    "    return result\n",
    "\n",
    "train_dataset=dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "validation_dataset=dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader= DataLoader(dataset=train_dataset,batch_size=2000,shuffle=True)\n",
    "validation_loader= DataLoader(dataset=validation_dataset,batch_size=5000,shuffle=False)\n",
    "\n",
    "criterion= CrossEntropyLoss()\n",
    "model= Net(layers)\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer= SGD(model.parameters(),lr=learning_rate) # momentum=0.4)\n",
    "\n",
    "training_loss, validation_accuracy = train(model,\n",
    "                                           criterion,\n",
    "                                           train_loader,\n",
    "                                           validation_loader,\n",
    "                                           optimizer,\n",
    "                                           epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Convolution](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)\n",
    "\n",
    "<img src = \"https://ibm.box.com/shared/static/wq8wbqhm4824y1oxpdbol55q645gykg9.gif\" width = 500, align = \"center\">\n",
    "\n",
    "> source: [cognitiveclass.ai](https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/DL0110EN/6.1.1What%20is%20Convolution.ipynb)\n",
    "\n",
    "+ [MaxPool2d](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d)\n",
    "+ kernels count = **in_channels**\n",
    "+ 1 bais\n",
    "\n",
    "+ [Algorithmia](https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ NumPy\n",
    "    + np.convolve(x, h, \"valid|same|full\")\n",
    "        + valid: no padding\n",
    "        + same: adds zeros to left (& top)\n",
    "        + full: padding\n",
    "        \n",
    "\n",
    "+ PyTorch\n",
    "\n",
    "```python\n",
    "    from torch.nn import Conv2d\n",
    "    from torch import tensor\n",
    "\n",
    "    conv1 = Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\n",
    "    conv1.state_dict()['weight'][0][0] = tensor([[1.0,1.0],[1.0,1.0]])\n",
    "    conv1.state_dict()['bias'][0] = 0.0\n",
    "    conv1.state_dict()\n",
    "\n",
    "    z1 = conv1(image1)\n",
    "```\n",
    "\n",
    "+ TensorFlow\n",
    "\n",
    "```python\n",
    "    input = tf.Variable(tf.random_normal([1, 10, 10, 1]))\n",
    "    filter = tf.Variable(tf.random_normal([3, 3, 1, 1]))\n",
    "    op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "```\n",
    "\n",
    "+ SciPy\n",
    "\n",
    "```python\n",
    "    from scipy import signal\n",
    "    input = tf.Variable(tf.random_normal([1, 10, 10, 1]))\n",
    "    filter = tf.Variable(tf.random_normal([3, 3, 1, 1]))\n",
    "    op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "\n",
    "#### [linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linspace.html)\n",
    "+ Return evenly spaced numbers over a specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import arange\n",
    "from numpy import linspace\n",
    "\n",
    "print( linspace(-2, 2 ,5) )\n",
    "print( arange(-2, 2 ,5).numpy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [array([]).T](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.ndarray.T.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "x = array([[1,2,3], [4, 5, 6]])\n",
    "print(x)\n",
    "print(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.meshgrid.html\n",
    "# https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.c_.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "#### torch.tensor( , [requires_grad=True, dtype=torch.int8|uint8|int16/short|half|float|int|double|long, device=cuda0])\n",
    "+ .zeros()\n",
    "+ .ones()\n",
    "+ .pow(2)\n",
    "+ .sum()\n",
    "+ .ndimension()\n",
    "+ .numpy()\n",
    "+ .shape\n",
    "+ .dtype\n",
    "+ [begin_row **\\:** end_row **\\,** begin_column **\\:** end_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import ones\n",
    "from torch import zeros\n",
    "\n",
    "print(zeros((2,)))\n",
    "print(ones((2,2)).numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False).view()](https://pytorch.org/docs/stable/torch.html#torch.arange)\n",
    "\n",
    "+ [reshape(input, shape)](https://pytorch.org/docs/stable/torch.html#torch.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import arange\n",
    "from torch import reshape\n",
    "\n",
    "print( arange(-2, 2, 1) ) # 1 Row\n",
    "\n",
    "print( arange(-2, 2, 1).view(-1, 1) ) # 1 Column\n",
    "print( reshape(arange(-2, 2, 1), (-1, 1)) ) # same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from torch import load\n",
    "\n",
    "save({\"a\": 123}, 'tmp.pt')\n",
    "tdict = load('tmp.pt')\n",
    "print(tdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative\n",
    "\n",
    "### Partial derivative w respect to u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "import torch.functional as F\n",
    "\n",
    "# Calculate f(u, v) = v * u + u^2 at u = 1, v = 2\n",
    "\n",
    "u = torch.tensor(1.0,requires_grad=True)\n",
    "v = torch.tensor(2.0,requires_grad=True)\n",
    "f = u * v + u ** 2\n",
    "\n",
    "f.backward()\n",
    "print(\"The result of v * u + u^2: \", f)\n",
    "print(\"The partial derivative with respect to u: \", u.grad)\n",
    "print(\"The partial derivative with respect to v: \", v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the derivative with multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-10, 10, 10, requires_grad = True)\n",
    "Y = x ** 2\n",
    "y = torch.sum(x ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [scikit](http://www.scikit-learn.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "\n",
    "You have two basic options when using TensorFlow to run your code [[ref](https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/ML0120EN/ML0120EN-2.2-Review-CNN-MNIST-Dataset.ipynb)]:\n",
    "- [Build graphs and run session] Do all the set-up and THEN execute a session to evaluate tensors and run operations (ops) \n",
    "- [Interactive session] create your coding and run on the fly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/ML0120EN/ML0120EN-1.4-Review-LogisticRegressionwithTensorFlow.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a graph\n",
    "graph1 = tf.Graph()\n",
    "\n",
    "# Variables must be initialized\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "v = tf.Variable(0)\n",
    "update = tf.assign(v, v+1)\n",
    "\n",
    "# Placeholder can feed data outside of a graph\n",
    "ph = tf.placeholder(tf.float32)\n",
    "\n",
    "# Loss\n",
    "a = tf.Variable(20.0)\n",
    "b = tf.Variable(30.2)\n",
    "y = a * train_x + b\n",
    "loss = tf.reduce_mean(tf.square(y - train_y))\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "\n",
    "# train\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "with graph1.as_default():\n",
    "    constant = tf.constant([2], name = 'constant_a') #return tf.Tensor object\n",
    "    session.run(init_op)\n",
    "    \n",
    "    \n",
    "# Interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Session for graph1\n",
    "sess = tf.Session(graph = graph1)\n",
    "result = sess.run(a)\n",
    "print(result)\n",
    "sess.close()\n",
    "\n",
    "# Session + Graph\n",
    "with tf.Session(graph = graph1) as sess:\n",
    "    result = sess.run(a)\n",
    "    print(result)\n",
    "    session.run(init_op)\n",
    "\n",
    "# Initialization\n",
    "numFeatures = trainX.shape[1]\n",
    "numLabels = trainY.shape[1]\n",
    "# Placeholders\n",
    "# 'None' means TensorFlow shouldn't expect a fixed number in that dimension\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels]) # This will be our correct answers matrix for 3 classes.\n",
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=0.01,\n",
    "                                       name=\"weights\"))\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=0.01,\n",
    "                                    name=\"bias\"))\n",
    "\n",
    "# Three-component breakdown of the Logistic Regression equation.\n",
    "# Note that these feed into each other.\n",
    "apply_weights_OP = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\") \n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
    "\n",
    "#Defining our cost function - Squared Mean Error\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-yGold, name=\"squared_error_cost\")\n",
    "\n",
    "# Number of Epochs in our training\n",
    "numEpochs = 700\n",
    "# Defining our learning rate iterations (decay)\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=trainX.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\n",
    "\n",
    "#Defining our Gradient Descent\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/ML0120EN/ML0120EN-3.1-Reveiw-LSTM-basics.ipynb\n",
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/ML0120EN/ML0120EN-3.2-Review-LSTM-LanguageModelling.ipynb\n",
    "from tensorflow import placeholder\n",
    "from tensorflow.nn import dynamic_rnn\n",
    "from tensorflow.contrib.rnn import LSTMCell, BasicLSTMCell, MultiRNNCell\n",
    "\n",
    "LSTM_CELL_SIZE = 4 #hidden nodes\n",
    "\n",
    "stacked_lstm = MultiRNNCell([\n",
    "                    BasicLSTMCell(LSTM_CELL_SIZE, forget_bias=0.0),\n",
    "                    LSTMCell(LSTM_CELL_SIZE),    \n",
    "                    LSTMCell(LSTM_CELL_SIZE) ])\n",
    "output, state = dynamic_rnn(stacked_lstm, data, dtype=tf.float32)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(output, feed_dict={data: sample_input}) # TODO fix feed_dict placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "z = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]])\n",
    "torch.nn.functional.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "import tensorflow as tf\n",
    "\n",
    "# https://labs.cognitiveclass.ai/tools/jupyterlab-for-ibm-powerai/lab/tree/labs/ML0122EN/ML0122EN-Project-LSTM-CharacterModelling-PowerAI.ipynb\n",
    "# to define a LSTM cell\n",
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.random_uniform\n",
    "tf.shape\n",
    "tf.reduce_mean #the mean of elements across dimensions of a tensor\n",
    "tf.square\n",
    "tf.nn.embedding_lookup\n",
    "tf.squeeze\n",
    "tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
