{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![install](\n",
    "https://anaconda.org/yoga1290/pytorch/badges/installer/ipynb.svg)](https://anaconda.org/yoga1290/pytorch/notebook) [![downloads](https://anaconda.org/yoga1290/pytorch/badges/downloads.svg)](https://anaconda.org/yoga1290/pytorch/notebook) [![Anaconda Cloud](\n",
    "https://anaconda.org/yoga1290/pytorch/badges/version.svg)](https://anaconda.org/yoga1290/pytorch/notebook)\n",
    "# NOTE: WIP, visit me later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "+ [Setup](#Environment-setup)\n",
    "+ **Training**: adjust W & b\n",
    "    + Initialization\n",
    "        + [Dataset & DataLoader](#Dataset)\n",
    "        + **Ecoph**: iteration in the training phase\n",
    "    + Forward Propagation\n",
    "        + [Model](#Models)\n",
    "            + **in_features**: size of W\n",
    "            + **out_features**: classes\n",
    "            + Linear Classifiers: returns positive & negative values\n",
    "            + Logistic Regression: returns [0 - 1] values\n",
    "            + Threshold function: returns either 0 or 1 \n",
    "            + [Linear Regression](#Linear-Regression)\n",
    "            + **RBM: Restricted Boltzman Machine**\n",
    "                + Shallow Neural Networks (2 layers)\n",
    "                + feature extraction\n",
    "                + **DBN: Deep Believe Network**\n",
    "                    + feature extraction is unsupervised by a stack of RBMs.\n",
    "                    + Solves the backward propagation, local minima & vanishing gradients\n",
    "            + **Recurrent Model/RNN**\n",
    "                + May use time-window to store state\n",
    "                + Vanishing/Exploding gradient may occurs\n",
    "                + **LSTM: Long Short Term Memory networks**\n",
    "                    + solves the vanishing/exploding gradient\n",
    "                    + Unfloaded LSTM\n",
    "                    + Stacked LSTM\n",
    "            + **Weight Initialization**\n",
    "                + Zeros\n",
    "                    + the derivative with respect to loss function is the same for every w; similar to linear model [read me](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "                + **Xavier initialization** [see](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "                    + Tanh\n",
    "                + He/Kaiming uniform initialization\n",
    "                    + ReLU\n",
    "                + Uniform distribution\n",
    "                + Normal distribution\n",
    "                    + Vanishing gradients\n",
    "                    + Exploding gradients\n",
    "                        + \"This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn\" [see](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "        + **Normalization**:\n",
    "            + \"Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information\" [see](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/normalize-data#module-overview)\n",
    "            + Batch Normalization\n",
    "                + \"batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\" [see](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c#b87b)\n",
    "                + Remove Dropout\n",
    "                + Reduce Internal Covariate Shift\n",
    "                + Increase learning rate\n",
    "                + Bias is not necessary\n",
    "                + USE `model.eval()` before perdication ($\\hat{y}$)\n",
    "        + **Regularization**\n",
    "            + Dropout\n",
    "                + USE `model.train()` before training\n",
    "                + USE `model.eval()` before perdication ($\\hat{y}$)\n",
    "        + **Activation functions**:\n",
    "            + Tanh\n",
    "                + zero centered [-1, 1]\n",
    "                + **Xavier initialization** [see](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "                + Vanishing gradient\n",
    "            + Sigmoid\n",
    "                + [0, 1]\n",
    "                + Initialization\n",
    "                + Vanishing gradient\n",
    "                + **Binary** classification\n",
    "            + ReLU\n",
    "                + [0, 1]\n",
    "                + \"With RELU(z) vanishing gradients are generally not a problem as the gradient is 0 for negative (and zero) inputs and 1 for positive inputs.\" [read me](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94)\n",
    "            + Softmax\n",
    "                + **Multi-class** classification\n",
    "                \n",
    "    + **Loss/Cost**: the difference between the prediected values,\n",
    "        $\\hat{y}$ and true labels, $y$\n",
    "        + [Derivative](#Derivative)\n",
    "        + [Mean Square Error](#Mean-Square-Error)\n",
    "        + [Binary Cross Entropy](#Binary-Cross-Entropy)\n",
    "        + [Cross Entropy](#Cross-Entropy)\n",
    "            + **Multi-class** \"This criterion expects a class index (0 to C-1) as the target for each value\" [PyTorch](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "    + **Backward propagation**:    \n",
    "        + **Optimization**: updates W & b in the Backward propagation\n",
    "            + [Adam optimizer](#Adam)\n",
    "            + Gradient Descent Optimization\n",
    "                + Batch Gradient Descent\n",
    "                + Mini-Batch Gradient Descent (PyTorch's **default**)\n",
    "                + Stochastic Gradient Descent\n",
    "                    + Update loss by one sample at a time\n",
    "                    + Sudden increases may occur\n",
    "                    + May not be accurate\n",
    "                    + Good for big data\n",
    "+ **Validation**: adjust the hyper-parameters; learning rate & batch size\n",
    "    + **Early Stopping**:\n",
    "        + \"Stop training when a monitored quantity has stopped improving\" [[tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)]\n",
    "        + or validation error just got worse\n",
    "\n",
    "+ Tools & Libraries\n",
    "    + **Visualization**\n",
    "        + Pandas\n",
    "        + Matplotlib\n",
    "    + [Numpy](#NumPy)\n",
    "+ Cheatsheets\n",
    "    + [ml-cheatsheet.readthedocs.io](https://ml-cheatsheet.readthedocs.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "\n",
    "## Install\n",
    "+ Install [Anaconda](https://www.anaconda.com/download/#linux)\n",
    "+ Expose `~/anaconda3/bin` (where `conda` executable biniary)\n",
    "+ Install [PyTorch](https://pytorch.org/): `conda install pytorch torchvision -c pytorch`\n",
    "\n",
    "## Online tools\n",
    "\n",
    "### [Google CoLaboratory](https://colab.research.google.com) [[open me!](https://colab.research.google.com/github/yoga1290/cheatsheets/blob/master/PyTorch.ipynb)]\n",
    "\n",
    "Install PyTorch on [Google CoLaboratory](https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb#scrollTo=RHXKNvj8ROgq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb#scrollTo=RHXKNvj8ROgq\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization\n",
    "\n",
    "+ [PUML](https://raw.githubusercontent.com/yoga1290/cheatsheets/master/gradient-descent.puml)\n",
    "![Gradient Descent](https://github.com/yoga1290/cheatsheets/raw/master/gradient-descent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import arange, randn\n",
    "\n",
    "# https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel#dataset\n",
    "class MyDataset(Dataset):\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.x = arange(-3, 3, 0.1).view(-1, 1)\n",
    "        self.f = 1 * self.x - 1\n",
    "        self.y = self.f + 0.1 * randn(self.x.size())\n",
    "        self.len = self.x.shape[0]\n",
    "        \n",
    "    # Getter\n",
    "    def __getitem__(self,index):    \n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    # Get Length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "dataLoader = DataLoader(MyDataset(), **params)\n",
    "\n",
    "# for X, y in dataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prebuilt dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "dataset = dsets.MNIST(\n",
    "    root = './data2', \n",
    "    train = False, \n",
    "    download = True, \n",
    "    transform = transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "def show_data(data_sample, shape = (28, 28)):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1].item()))\n",
    "\n",
    "show_data(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchvision Transforms\n",
    "\n",
    "+ Compose\n",
    "+ CenterCrop\n",
    "+ ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "croptensor_data_transform = Compose([\n",
    "    CenterCrop(20),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# set train = false for validation\n",
    "dataset = dsets.MNIST(root = './data', train = False, download = True, transform = croptensor_data_transform)\n",
    "print(\"The shape of the first element in the first tuple: \", dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "![source: https://youtu.be/zJSY2C9xzoU](https://github.com/yoga1290/cheatsheets/raw/4346bd63cf490d978cee156497db3b64f87fc37e/resources/nn-3.3.2-zJSY2C9xzoU.png)\n",
    "![source: https://youtu.be/zJSY2C9xzoU](https://github.com/yoga1290/cheatsheets/raw/4346bd63cf490d978cee156497db3b64f87fc37e/resources/nn-3.3.2-2-zJSY2C9xzoU.png)\n",
    "![source: https://youtu.be/xR4Ian1UIGM](https://github.com/yoga1290/cheatsheets/raw/4346bd63cf490d978cee156497db3b64f87fc37e/resources/nn2-xR4Ian1UIGM.png)\n",
    "> sources [[1](https://youtu.be/xR4Ian1UIGM)] [[2](https://youtu.be/xR4Ian1UIGM)] [[3](https://youtu.be/xR4Ian1UIGM)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn.[Model](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear, Dropout\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "# Customize Linear Regression Class\n",
    "class linear_regression(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # Inherit from parent\n",
    "        super(linear_regression, self).__init__()\n",
    "        self.linear = Linear(in_features, out_features, bias=True) #TODO\n",
    "    def forward(self, x):\n",
    "        yhat = self.linear(x)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.init.kaiming_uniform_(linear.weight,nonlinearity='relu')\n",
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/DL0110EN/5.1.2.He_Initialization.ipynb\n",
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/DL0110EN/5.3.1BachNorm.ipynb\n",
    "\n",
    "from torch.nn import Module, ModuleList, Linear, Dropout, BatchNorm1d\n",
    "from torch.nn.init import kaiming_uniform_, xavier_uniform_\n",
    "from torch.nn.functional import relu, tanh\n",
    "\n",
    "class Net(Module):\n",
    "    # Constructor\n",
    "    # in_features = len(W)\n",
    "    def __init__(self,Layers, p=0):\n",
    "        # Inherit from parent\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden = ModuleList()\n",
    "        self.drop = Dropout(p=p)\n",
    "\n",
    "        for input_size,output_size in zip(Layers,Layers[1:]):\n",
    "            linear = Linear(input_size,output_size)\n",
    "            \n",
    "            # Uniform initialization\n",
    "            #linear.weight.data.uniform_(0, 1)\n",
    "            \n",
    "            # He/Kaiming uniform initialization\n",
    "            #kaiming_uniform_(linear.weight, nonlinearity='relu')\n",
    "            \n",
    "            # Xavier initialization\n",
    "            #xavier_uniform_(linear.weight)\n",
    "            \n",
    "            self.hidden.append( linear )\n",
    "            \n",
    "            # Batch Normalization\n",
    "            # self.hidden.append( BatchNorm1d(output_size) )\n",
    "            \n",
    "            # Dropout\n",
    "            # self.hidden.append( Dropout(p=p) )\n",
    "\n",
    "    # Prediction function\n",
    "    # https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "    def forward(self,x):\n",
    "        L=len(self.hidden)\n",
    "        for (l, linear_transform)  in zip(range(L),self.hidden):\n",
    "            if l<L-1:\n",
    "                x = relu(linear_transform (x))\n",
    "                #x = tanh(linear_transform (x))\n",
    "                #x = self.drop(x)\n",
    "            # last layer\n",
    "            else:\n",
    "                x =linear_transform (x)\n",
    "        \n",
    "        return x #yhat\n",
    "    \n",
    "    #def activation(self,x):\n",
    "    #    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Linear, Sigmoid\n",
    "\n",
    "model = Sequential( Linear(2,1), Sigmoid() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### state_dict(), load_state_dict(dict), save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from torch import load\n",
    "from torch.nn import Module, Linear\n",
    "\n",
    "save({\"a\": 123}, 'tmp.pt')\n",
    "tdict = load('tmp.pt')\n",
    "print(tdict)\n",
    "\n",
    "model = Linear(5, 1)\n",
    "save(model.state_dict(), 'model.pt')\n",
    "model.load_state_dict( load('model.pt') )\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import linspace, tensor\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "x = linspace(-3, 3, 100, requires_grad = True)\n",
    "Y = relu(x)\n",
    "\n",
    "z = tensor([[1,0,-1],[2,0,-2],[1,0,-1]])\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterion/Cost/Loss\n",
    "\n",
    "Comparing/differentiating the prediected values (**Y^**) and the actual labels (**Y**)\n",
    "\n",
    "### Mean Square Error\n",
    "\n",
    "+ [torch.nn.MSELoss(size_average=None, reduce=None, reduction='elementwise_mean')](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "criterion = MSELoss()\n",
    "\n",
    "# equivalent to:\n",
    "from torch import mean\n",
    "\n",
    "def criterion(yhat, y):\n",
    "    return mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "\n",
    "criterion = BCELoss()\n",
    "\n",
    "# equivalent to:\n",
    "from torch import mean\n",
    "from torch import log\n",
    "\n",
    "def criterion(yhat, y):\n",
    "    return -1 * mean(y * log(yhat) + (1-y) * log(1 - yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "# https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "\n",
    "criterion = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "opt = Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train([true]) # sets model.training = true\n",
    "\n",
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/DL0110EN/5.1.1Xaviermist1layer.ipynb\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model,criterion, train_loader,validation_loader, optimizer, epochs=100):\n",
    "    i=0\n",
    "    useful_stuff={'training_loss':[],'validation_accuracy':[]}  \n",
    "    \n",
    "    #n_epochs\n",
    "    for epoch in range(epochs):\n",
    "        for i,(x, y) in enumerate(train_loader):\n",
    "\n",
    "            #clear gradient \n",
    "            optimizer.zero_grad()\n",
    "            #make a prediction logits \n",
    "            z=model(x.view(-1,28*28))\n",
    "            # calculate loss \n",
    "            loss=criterion(z,y)\n",
    "    \n",
    "            # calculate gradients of parameters \n",
    "            loss.backward()\n",
    "            # update parameters \n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.data.item())\n",
    "        correct=0\n",
    "        for x, y in validation_loader:\n",
    "            #perform a prediction on the validation  data  \n",
    "            yhat= model(x.view(-1,28*28))\n",
    "            \n",
    "            _,lable=torch.max(yhat,1)\n",
    "            correct+=(lable==y).sum().item()\n",
    " \n",
    "    \n",
    "        accuracy=100*(correct/len(validation_dataset))\n",
    "   \n",
    "        useful_stuff['validation_accuracy'].append(accuracy)\n",
    "    \n",
    "    return useful_stuff\n",
    "\n",
    "train_dataset=dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "validation_dataset=dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader= DataLoader(dataset=train_dataset,batch_size=2000,shuffle=True)\n",
    "validation_loader= DataLoader(dataset=validation_dataset,batch_size=5000,shuffle=False)\n",
    "\n",
    "criterion= CrossEntropyLoss()\n",
    "model= Net(layers)\n",
    "optimizer= SGD(model.parameters(),lr=learning_rate) # momentum=0.4)\n",
    "\n",
    "learning_rate=0.01\n",
    "training_results=train(model,criterion, train_loader,validation_loader, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Convolution](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)\n",
    "\n",
    "<img src = \"https://ibm.box.com/shared/static/wq8wbqhm4824y1oxpdbol55q645gykg9.gif\" width = 500, align = \"center\">\n",
    "\n",
    "> source: [cognitiveclass.ai](https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/DL0110EN/6.1.1What%20is%20Convolution.ipynb)\n",
    "\n",
    "+ [MaxPool2d](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d)\n",
    "+ kernels count = **in_channels**\n",
    "+ 1 bais\n",
    "\n",
    "+ [Algorithmia](https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "from torch import tensor\n",
    "\n",
    "conv1 = Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\n",
    "conv1.state_dict()['weight'][0][0] = tensor([[1.0,1.0],[1.0,1.0]])\n",
    "conv1.state_dict()['bias'][0] = 0.0\n",
    "conv1.state_dict()\n",
    "\n",
    "z1 = conv1(image1)\n",
    "\n",
    "print(\"z4:\",z4)\n",
    "print(\"z4:\",z4.shape[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "\n",
    "#### [linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linspace.html)\n",
    "+ Return evenly spaced numbers over a specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import arange\n",
    "from numpy import linspace\n",
    "\n",
    "print( linspace(-2, 2 ,5) )\n",
    "print( arange(-2, 2 ,5).numpy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [array([]).T](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.ndarray.T.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "x = array([[1,2,3], [4, 5, 6]])\n",
    "print(x)\n",
    "print(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.meshgrid.html\n",
    "# https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.c_.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "#### torch.tensor( , [requires_grad=True, dtype=torch.int8|uint8|int16/short|half|float|int|double|long, device=cuda0])\n",
    "+ .zeros()\n",
    "+ .ones()\n",
    "+ .pow(2)\n",
    "+ .sum()\n",
    "+ .ndimension()\n",
    "+ .numpy()\n",
    "+ .shape\n",
    "+ .dtype\n",
    "+ [begin_row **\\:** end_row **\\,** begin_column **\\:** end_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import ones\n",
    "from torch import zeros\n",
    "\n",
    "print(zeros((2,)))\n",
    "print(ones((2,2)).numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False).view()](https://pytorch.org/docs/stable/torch.html#torch.arange)\n",
    "\n",
    "+ [reshape(input, shape)](https://pytorch.org/docs/stable/torch.html#torch.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import arange\n",
    "from torch import reshape\n",
    "\n",
    "print( arange(-2, 2, 1) ) # 1 Row\n",
    "\n",
    "print( arange(-2, 2, 1).view(-1, 1) ) # 1 Column\n",
    "print( reshape(arange(-2, 2, 1), (-1, 1)) ) # same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import save\n",
    "from torch import load\n",
    "\n",
    "save({\"a\": 123}, 'tmp.pt')\n",
    "tdict = load('tmp.pt')\n",
    "print(tdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative\n",
    "\n",
    "### Partial derivative w respect to u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "import torch.functional as F\n",
    "\n",
    "# Calculate f(u, v) = v * u + u^2 at u = 1, v = 2\n",
    "\n",
    "u = torch.tensor(1.0,requires_grad=True)\n",
    "v = torch.tensor(2.0,requires_grad=True)\n",
    "f = u * v + u ** 2\n",
    "\n",
    "f.backward()\n",
    "print(\"The result of v * u + u^2: \", f)\n",
    "print(\"The partial derivative with respect to u: \", u.grad)\n",
    "print(\"The partial derivative with respect to v: \", v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the derivative with multiple values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-10, 10, 10, requires_grad = True)\n",
    "Y = x ** 2\n",
    "y = torch.sum(x ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [scikit](http://www.scikit-learn.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "+ NumPy\n",
    "    + np.convolve(x, h, \"valid|same|full\")\n",
    "        + valid: no padding\n",
    "        + same: adds zeros to left (& top)\n",
    "        + full: padding\n",
    "        \n",
    "\n",
    "+ PyTorch\n",
    "\n",
    "```python\n",
    "    from torch.nn import Conv2d\n",
    "    from torch import tensor\n",
    "\n",
    "    conv1 = Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\n",
    "    conv1.state_dict()['weight'][0][0] = tensor([[1.0,1.0],[1.0,1.0]])\n",
    "    conv1.state_dict()['bias'][0] = 0.0\n",
    "    conv1.state_dict()\n",
    "\n",
    "    z1 = conv1(image1)\n",
    "```\n",
    "\n",
    "+ TensorFlow\n",
    "\n",
    "```python\n",
    "    input = tf.Variable(tf.random_normal([1, 10, 10, 1]))\n",
    "    filter = tf.Variable(tf.random_normal([3, 3, 1, 1]))\n",
    "    op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "```\n",
    "\n",
    "+ SciPy\n",
    "\n",
    "```python\n",
    "    from scipy import signal\n",
    "    input = tf.Variable(tf.random_normal([1, 10, 10, 1]))\n",
    "    filter = tf.Variable(tf.random_normal([3, 3, 1, 1]))\n",
    "    op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    op2 = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gfdv8046/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'self' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c298ea32c884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mconstant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'constant_a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#return tf.Tensor object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    540\u001b[0m     raise TypeError(\n\u001b[1;32m    541\u001b[0m         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0mappend_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtensorflow/python/framework/fast_tensor_util.pyx\u001b[0m in \u001b[0;36mtensorflow.python.framework.fast_tensor_util.AppendInt32ArrayToTensorProto\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m\"\"\"Appends an item to the list. Similar to list.append().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'self' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# https://labs.cognitiveclass.ai/tools/jupyterlab/lab/tree/labs/ML0120EN/ML0120EN-1.4-Review-LogisticRegressionwithTensorFlow.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a graph\n",
    "graph1 = tf.Graph()\n",
    "\n",
    "# Variables must be initialized\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "v = tf.Variable(0)\n",
    "update = tf.assign(v, v+1)\n",
    "\n",
    "# Placeholder can feed data outside of a graph\n",
    "ph = tf.placeholder(tf.float32)\n",
    "\n",
    "# Loss\n",
    "a = tf.Variable(20.0)\n",
    "b = tf.Variable(30.2)\n",
    "y = a * train_x + b\n",
    "loss = tf.reduce_mean(tf.square(y - train_y))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "\n",
    "# train\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "with graph1.as_default():\n",
    "    constant = tf.constant([2], name = 'constant_a') #return tf.Tensor object\n",
    "    session.run(init_op)\n",
    "    \n",
    "    \n",
    "# Session for graph1\n",
    "sess = tf.Session(graph = graph1)\n",
    "result = sess.run(a)\n",
    "print(result)\n",
    "sess.close()\n",
    "\n",
    "# Session + Graph\n",
    "with tf.Session(graph = graph1) as sess:\n",
    "    result = sess.run(a)\n",
    "    print(result)\n",
    "    session.run(init_op)\n",
    "\n",
    "# Initialization\n",
    "numFeatures = trainX.shape[1]\n",
    "numLabels = trainY.shape[1]\n",
    "# Placeholders\n",
    "# 'None' means TensorFlow shouldn't expect a fixed number in that dimension\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "yGold = tf.placeholder(tf.float32, [None, numLabels]) # This will be our correct answers matrix for 3 classes.\n",
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=0.01,\n",
    "                                       name=\"weights\"))\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=0.01,\n",
    "                                    name=\"bias\"))\n",
    "\n",
    "# Three-component breakdown of the Logistic Regression equation.\n",
    "# Note that these feed into each other.\n",
    "apply_weights_OP = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\") \n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
    "\n",
    "#Defining our cost function - Squared Mean Error\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-yGold, name=\"squared_error_cost\")\n",
    "\n",
    "# Number of Epochs in our training\n",
    "numEpochs = 700\n",
    "# Defining our learning rate iterations (decay)\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.0008,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=trainX.shape[0],\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\n",
    "\n",
    "#Defining our Gradient Descent\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "z = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]])\n",
    "torch.nn.functional.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
